%%
%% Copyright 2007-2025 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%% $Id: elsarticle-template-num.tex 272 2025-01-09 17:36:26Z rishi $
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

%\journal{Nuclear Physics B}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

\title{Neural Network-Based Research Interest Prediction from Academic Publications}

%% Author name
\author{Yuteng Huang}

%% Author affiliation
\affiliation{}

%% Abstract
\begin{abstract}
The growth of academic publications necessitates automated systems for researcher interest profiling.
This paper presents a neural network-based approach for predicting scholars' research interests by analyzing their publication metadata.
Our system processes 500k research papers, employing TF-IDF feature engineering with author position weighting and citation analysis.
We design a deep neural network architecture with soft label encoding to capture multi-interest preferences.
Experimental results demonstrate competitive performance, achieving 31.33\% top-5 accuracy.
\end{abstract}

% %%Graphical abstract
% \begin{graphicalabstract}
% %\includegraphics{grabs}
% \end{graphicalabstract}

% %%Research highlights
% \begin{highlights}
% \item Research highlight 1
% \item Research highlight 2
% \end{highlights}

%% Keywords
\begin{keyword}
research interest prediction \sep academic profiling \sep neural networks \sep TF-IDF \sep natural language processing
\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment
%% following line to enable line numbers
%% \linenumbers

%% main text
%%
\section{Introduction}
\label{sec1}

The rapid growth of academic publications has created a pressing need for automated systems that can accurately identify researchers' interests based on their publication records.
Traditional manual categorization methods are time-consuming and subjective, making automated approaches essential for large-scale academic profiling.
This paper presents a neural network-based system for predicting scholars' research interests by analyzing their paper metadata, including titles, venues, and authorship information.

The core challenge lies in transforming heterogeneous academic data into meaningful feature representations that capture the essence of a researcher's work.
Our approach leverages natural language processing techniques and deep learning to build a robust multi-label classification system that can predict researchers' top interests with competitive accuracy.

\section{Data Processing and Feature Engineering}
\label{sec2}

\subsection{Dataset Description}
Our system utilizes a comprehensive academic dataset comprising 500k research papers with detailed metadata.
The training set contains 6,000 author-interest pairs, while the validation set includes 4,000 samples. Each author is associated with multiple research interests ranked by relevance.

The dataset structure includes:
\begin{itemize}
    \item Paper metadata: authors, title, publication year, venue, citations
    \item Author-interest mappings with preference rankings
    \item 789 unique research interest labels
\end{itemize}

\subsection{Text Preprocessing}
We implement comprehensive text preprocessing to clean and normalize the academic text data:

\begin{verbatim}
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    text = ' '.join(text.split())
    return text
\end{verbatim}

This preprocessing pipeline converts text to lowercase, removes special characters, and eliminates extra whitespace, ensuring consistent feature representation.

\subsection{Feature Extraction}
We employ TF-IDF vectorization with enhanced feature engineering:

\begin{verbatim}
vectorizer = TfidfVectorizer(
    max_features=2048,
    min_df=16,
    max_df=0.8,
    stop_words='english',
    ngram_range=(1, 2)
)
\end{verbatim}

Key innovations in our feature engineering include:
\begin{itemize}
    \item \textbf{Author position weighting}: First authors receive higher weights (1.0) with linear decay for subsequent authors
    \item \textbf{Citation analysis}: Incorporate cited paper content with weighted importance
    \item \textbf{Journal normalization}: Treat journal names as single tokens to capture venue significance
\end{itemize}

The final feature matrix has dimensions 6,000 × 2,048 for training and 4,000 × 2,048 for validation.

\section{Model Architecture and Training}
\label{sec3}

\subsection{Neural Network Design}
We implement a deep neural network with the following architecture:

\begin{verbatim}
model = Sequential([
    Dense(2048, activation='relu', input_shape=(2048,)),
    BatchNormalization(),
    Dropout(0.4),
    Dense(512, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(789, activation='sigmoid')
])
\end{verbatim}

The model employs:
\begin{itemize}
    \item Input layer matching TF-IDF feature dimensions (2,048)
    \item Hidden layers with ReLU activation and batch normalization
    \item Dropout regularization (40\% and 30\%) to prevent overfitting
    \item Output layer with sigmoid activation for multi-label classification
\end{itemize}

\subsection{Soft Label Encoding}
We implement a novel soft label encoding scheme to capture interest preferences:

\begin{verbatim}
def create_soft_labels(train_df, all_labels):
    y_matrix = 0.05 * np.ones((num_authors, num_labels))
    for idx, row in train_df.iterrows():
        interests = row['interests']
        for interest_idx, interest in enumerate(interests):
            score = 1.0 - 0.1 * interest_idx
            y_matrix[idx, label_idx] = score
\end{verbatim}

This approach assigns:
\begin{itemize}
    \item Primary interest: score 1.0
    \item Secondary interest: score 0.9
    \item Tertiary interest: score 0.8
    \item Baseline for all labels: score 0.05
\end{itemize}

\subsection{Training Configuration}
The model is trained with the following hyperparameters:
\begin{itemize}
    \item Optimizer: Adam with learning rate 0.002
    \item Loss function: Binary cross-entropy
    \item Batch size: 32
    \item Epochs: 20
    \item Validation split: 20\%
\end{itemize}

\section{Experimental Results}
\label{sec4}

\subsection{Evaluation Metrics}
We evaluate our system using top-k accuracy metrics:
\begin{itemize}
    \item \textbf{Top-1 Accuracy}: Primary interest prediction accuracy
    \item \textbf{Top-5 Accuracy}: Interest presence in top 5 predictions
    \item Per-interest breakdown for detailed analysis
\end{itemize}

\subsection{Performance Analysis}
The system achieves competitive performance on the validation set:

\begin{table}[h]
\centering
\begin{tabular}{l c c}
\hline
Metric & Overall & Per-Interest Average \\
\hline
Top-1 Accuracy & 11.92\% & [21.73\%, 9.43\%, 4.62\%] \\
Top-5 Accuracy & 31.33\% & [45.32\%, 29.83\%, 18.85\%] \\
\hline
\end{tabular}
\caption{Model Performance on Validation Set}
\label{tab1}
\end{table}

Key observations from the results:
\begin{itemize}
    \item Strong performance on primary interest prediction (21.73\% top-1)
    \item Significant improvement with top-5 consideration (45.32\% for primary interest)
    \item Performance degrades for lower-ranked interests, reflecting dataset characteristics
\end{itemize}

\subsection{Training Dynamics}
The training process shows stable convergence:
\begin{itemize}
    \item Final training accuracy: 39.08\%
    \item Final validation accuracy: 21.25\%
    \item Consistent loss reduction throughout training
    \item Minimal overfitting due to effective regularization
\end{itemize}

\section{Conclusion}
\label{sec5}

We have developed a comprehensive neural network-based system for automated research interest prediction from academic publications. Our approach demonstrates several key contributions:

\textbf{Methodological Innovations}:
\begin{itemize}
    \item Enhanced TF-IDF feature engineering with author position weighting
    \item Sophisticated soft label encoding for multi-interest preference modeling
    \item Robust neural architecture with effective regularization techniques
\end{itemize}

\textbf{Practical Impact}:
\begin{itemize}
    \item Achieves 31.33\% top-5 accuracy on challenging multi-label classification
    \item Successfully processes 500,000 paper records efficiently
    \item Provides interpretable interest predictions with confidence scores
\end{itemize}

The system shows particular strength in identifying primary research interests, making it valuable for academic profiling, recommendation systems, and research trend analysis. Future work could explore transformer-based architectures and incorporate additional metadata such as abstracts and citation networks to further improve prediction accuracy.

\bibliographystyle{elsarticle-num}
\bibliography{cas-refs}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
